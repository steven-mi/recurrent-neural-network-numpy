{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\">Introduction</a></span></li><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-2\">Requirements</a></span><ul class=\"toc-item\"><li><span><a href=\"#Knowledge\" data-toc-modified-id=\"Knowledge-2.1\">Knowledge</a></span></li><li><span><a href=\"#Python-Modules\" data-toc-modified-id=\"Python-Modules-2.2\">Python Modules</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2.3\">Data</a></span></li></ul></li><li><span><a href=\"#Recurrent-neural-network\" data-toc-modified-id=\"Recurrent-neural-network-3\">Recurrent neural network</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-3.1\">Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forward-pass\" data-toc-modified-id=\"Forward-pass-3.1.1\">Forward pass</a></span></li><li><span><a href=\"#Backward-path\" data-toc-modified-id=\"Backward-path-3.1.2\">Backward path</a></span></li><li><span><a href=\"#Sampling\" data-toc-modified-id=\"Sampling-3.1.3\">Sampling</a></span></li><li><span><a href=\"#Trainings-process\" data-toc-modified-id=\"Trainings-process-3.1.4\">Trainings process</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter\" data-toc-modified-id=\"Hyperparameter-3.1.4.1\">Hyperparameter</a></span></li><li><span><a href=\"#Main\" data-toc-modified-id=\"Main-3.1.4.2\">Main</a></span></li></ul></li><li><span><a href=\"#Learning-curve\" data-toc-modified-id=\"Learning-curve-3.1.5\">Learning curve</a></span></li></ul></li></ul></li><li><span><a href=\"#Licenses\" data-toc-modified-id=\"Licenses-4\">Licenses</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notebook-License-(CC-BY-SA-4.0)\" data-toc-modified-id=\"Notebook-License-(CC-BY-SA-4.0)-4.1\">Notebook License (CC-BY-SA 4.0)</a></span></li><li><span><a href=\"#Code-License-(MIT)\" data-toc-modified-id=\"Code-License-(MIT)-4.2\">Code License (MIT)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will go over a vanilla recurrent neural network with a softmax classifier by looking at a numpy implementation of it.\n",
    "This implementation is based on the Lecture of [cs231 Recurrent Neural Network](https://www.youtube.com/watch?v=6niqTuYFZLQ) and [Karpathy's min char example](https://gist.github.com/karpathy/d4dee566867f8291f086). It is recommended to have a understanding for backpropagation and matrix calculus since we will use it but not go over it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Knowledge\n",
    "\n",
    "- [Recommended] [Neural Network and Deep Learning - Backpropagation](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "- [Recommended] [Matrix Calculus](https://explained.ai/matrix-calculus/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Als rekurrente bzw. rückgekoppelte neuronale Netze bezeichnet man neuronale Netze, die sich im Gegensatz zu den Feedforward-Netzen durch Verbindungen von Neuronen einer Schicht zu Neuronen derselben oder einer vorangegangenen Schicht auszeichnen. Im Gehirn ist dies die bevorzugte Verschaltungsweise neuronaler Netze, insbesondere im Neocortex. In künstlichen neuronalen Netzen werden rekurrente Verschaltung von Modellneuronen benutzt, um zeitlich codierte Informationen in den Daten zu entdecken\"\n",
    "\n",
    "text_length = len(text)\n",
    "chars = list(set(text))\n",
    "char_length = len(chars)\n",
    "\n",
    "# dictionaries which we will use in the future for transformations\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# transform the text string to a list of integers\n",
    "X = np.array([char_to_int[char] for char in text])\n",
    "\n",
    "print('text:\\n', text, '\\n')\n",
    "print('length of text:\\n', text_length, '\\namount of characters:\\n', char_length)\n",
    "print('alphabet:\\n', chars,'\\n')\n",
    "print('first 10 datas:\\n', X[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural network\n",
    "\n",
    "In traditional neural networks (e.g. convolution) we have to assume that all inputs are independent of each other. The network classifies each image separately and does not care about the image which was classified before. But for some tasks thats not ideal: if you want to predict the next word in a sentence you want to know which words came before it. Recurrent neural networks on the other hand are capable of using sequential informations because they perform the same task for every element of a sequence, with the output being depended on the previous calculations. You can think of Recurrent Neural Networks as Neural Networks with a in build memory or storage in which they can store informations about which calculations the neural network make. In theory RNNs can make use of information in arbitrarily long sequences, but in practice they are limited to looking back only a few steps. A simple RNN can be displayed as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/vanilla-rnn.jpg\" width=\"550px\"> \n",
    "- $n$ is the size of your sequence which you look at, at each iteration.\n",
    "- $x_n$ is one part of a sequence which you feed in the network, e.g. if we have a sequence $x = \\text{Hello Word}$ then $x_0 = \\text{Hello}$ and $x_1 = \\text{Word}$ is possible. In our implementation we feed in only 1 character at a time which would give us $x_0 = \\text{H}$, $x_1 = \\text{e}$, $x_2 = \\text{l}$, $x_4 = \\text{l}$, $x_4 = \\text{o}$, ....\n",
    "- $h_n$ is our memory which is called the hidden state of our recurrent neural network. You can see here that the hidden state of $h_{n-1}$ goes to $h_{n}$.\n",
    "- $W_hidden$ is the weight of our hidden state layer\n",
    "- $y_n$ is the prediction or output our network made. This layer is always different and depending on the task we want to do. If we want to build a RNN to predict the next character, then $y_n$ could be the output of a softmax\n",
    "- $W_{hy}$ is the weight of our output layer\n",
    "\n",
    "**Note:** You may wonder why we don't put a loss function to our diagram. This is because with RNN you can decide where you want to put it. One possibility is to have a loss value for each prediction, another you could imagine only having a loss value for the last 10 predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Here we go over the core functions of a recurrent neural network. Every function could be added to a ```rnn.py``` script to run the recurrent neural network outside of a notebook. This implementation is not efficient though and it is not recommended to use it on a very large text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "zz_t &= W_{xh} * x_t + W_{hh} * h_{t-1} \\\\\n",
    "h_t &= tanh(z_t) \\\\\n",
    "y_t &= W_{hy} * h_t \\\\\n",
    "p_t &=\\frac{e^{y_t}}{\\sum_k e^{y_k}} \\\\\n",
    "L &= \\sum_t - log (p_t) \\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, Y, h):\n",
    "    '''\n",
    "    This is the forward path of our recurrent neural network. It gets the data, labels and the current hidden state\n",
    "    and returns the new hidden date, a probability distribution of the data and the softmax loss\n",
    "    Args:\n",
    "        X: data as list of integers\n",
    "        Y: labels as list of integers\n",
    "        h: current hidden state as list\n",
    "\n",
    "    Returns:\n",
    "        (h, p, loss) - new hidden state as list, probability distribution as list, loss as number\n",
    "\n",
    "    '''\n",
    "    # initializes the variables which will be returned later\n",
    "    h, p, loss = [h[-1]], [], 0    \n",
    "    for t in range(len(Y)):\n",
    "        # transforming the a datap qoint at a time step to a one hot encoded vector\n",
    "        xt = np.zeros((char_length, 1))\n",
    "        xt[X[t]] = 1\n",
    "        # calculating forward pass based on the formular \n",
    "        z = np.tanh(np.dot(Wxh, xt) + np.dot(Whh, h[t]))\n",
    "        h.append(z)\n",
    "        y = np.dot(Why, h[t + 1])\n",
    "        p.append(np.exp(y) / np.sum(np.exp(y)))\n",
    "        loss += -np.sum(np.log(p[t][Y[t], 0]))\n",
    "    return h[1:], p, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward path\n",
    "\n",
    "$$\n",
    "\\begin{array}\n",
    "{}\\frac{\\partial L_t}{\\partial W_{hy}} &= (p_t - label_t) *h_t^T \\\\\n",
    "\\frac{\\partial L_t}{\\partial h_t} &= (p_t - label_t) * W_{hy}^T\\\\\n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}} &= \\frac{\\partial h_t}{\\partial z_t} \\frac{\\partial z_t}{\\partial h_{t-1}} = (1 - h_t^2)* W_{hh}^T \\\\\n",
    "\\frac{\\partial z_t}{\\partial W_{hh}} &= h_{t-1}^T \\\\\n",
    "\\frac{\\partial z_t}{\\partial W_{xh}} &= x_t^T\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, Y, h, p):\n",
    "    '''\n",
    "    Calculates the gradient of our recurrent neutral network.\n",
    "\n",
    "    Args:\n",
    "        X: data as list of integers\n",
    "        Y: labels as list of integers\n",
    "        h: current hidden state as list\n",
    "        p: probability distribution which was calculated at the forward path\n",
    "\n",
    "    Returns:\n",
    "        (dWhh, dWxh, dWhy) - (gradient of Whh, gradient of Wxh, gradient of Why)\n",
    "    '''\n",
    "\n",
    "    # initializes the gradients with zeros\n",
    "    dWhh, dWxh, dWhy = np.zeros_like(Whh), np.zeros_like(Wxh), np.zeros_like(Why)\n",
    "    # initializing our gradient dhprevious which is dh_t/dh_{t-1}\n",
    "    dhprevious = np.zeros_like(h[0])\n",
    "    for t in reversed(range(len(Y))):\n",
    "        # transforming the a datapoint at a time step to a one hot encoded vector\n",
    "        xt = np.zeros((char_length, 1))\n",
    "        xt[X[t]] = 1\n",
    "        # gradient of Why\n",
    "        dy = np.copy(p[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        dWhy += np.dot(dy, h[t].T)\n",
    "        # gradient of Wxh and Whh\n",
    "        dh = np.dot(Why.T, dy) + dhprevious  # backprop into h\n",
    "        dz = (1 - h[t] ** 2) * dh  # backprop through tanh nonlinearity\n",
    "        dWxh += np.dot(dz, xt.T)\n",
    "        dWhh += np.dot(dz, h[t - 1].T)\n",
    "        dhprevious = np.dot(Whh.T, dz)\n",
    "\n",
    "    # gradient clip to mitigate exploding gradients\n",
    "    for dparam in [dWxh, dWhh, dWhy]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    return dWhh, dWxh, dWhy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(start_char, h, n):\n",
    "    '''\n",
    "    This functions returns a sentence of length n based on the starting character, the latest hidden state.\n",
    "    \n",
    "    Args:\n",
    "        start_char: the character which we start with as integer\n",
    "        h: latest hidden state\n",
    "        n: length of sentence as integer\n",
    "\n",
    "    Returns:\n",
    "        A sample sentence as String\n",
    "    '''\n",
    "\n",
    "    # transforming the a starting character to a one hot encoded vector\n",
    "    x = np.zeros((char_length, 1))\n",
    "    x[start_char] = 1\n",
    "    # initializing output\n",
    "    text = ''\n",
    "    for t in range(n):\n",
    "        # predicting which character will be next\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h))\n",
    "        y = np.dot(Why, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # adds the predicted character to our output String\n",
    "        text += chars[np.argmax(p)]\n",
    "        # generates a random sample/new character from a given array based on a probability distributipn p\n",
    "        random_index = np.random.choice(range(char_length), p=p.ravel())\n",
    "        # transforming the generated character to a one hot encoded vector\n",
    "        x = np.zeros((char_length, 1))\n",
    "        x[random_index] = 1\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainings process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many characters it will look at, at each step\n",
    "seq_size = 20\n",
    "\n",
    "# size of the hidden layer \n",
    "hidden_size = 100\n",
    "\n",
    "# learning rate for the gradient descent algorithm\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# how many times the model sees the whole data\n",
    "epochs = 500\n",
    "\n",
    "print('sequence size', seq_size, '\\nhidden size:', hidden_size, '\\nlearning rate:', learning_rate, '\\n epochs:', epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing weights\n",
    "Wxh = np.random.randn(hidden_size, char_length) * 0.01\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Why = np.random.randn(char_length, hidden_size) * 0.01\n",
    "\n",
    "# initializing hidden state, loss history, squared gradient and loss\n",
    "grad_squared_xh, grad_squared_hh, grad_squared_hy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "loss_history = []\n",
    "h = [np.zeros((hidden_size, 1))]\n",
    "loss = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    h = [np.zeros((hidden_size, 1))]\n",
    "\n",
    "    for steps in range(0, text_length, seq_size):\n",
    "        # splits the data to the right sequence size\n",
    "        inputs = X[steps:steps+seq_size]\n",
    "        targets = X[steps+1:steps+1+seq_size]\n",
    "        # calculate the new hidden state, probability distribution and loss\n",
    "        h, p, loss = forward_pass(inputs, targets, h)\n",
    "        loss_history.append(loss)\n",
    "        # get the gradients\n",
    "        dWhh, dWxh, dWhy = backward_pass(inputs, targets, h, p)\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        for param, dparam, mem in zip([Wxh, Whh, Why],\n",
    "                                      [dWxh, dWhh, dWhy],\n",
    "                                      [grad_squared_xh, grad_squared_hh, grad_squared_hy]):\n",
    "                mem += dparam * dparam\n",
    "                param += -learning_rate * dparam / \\\n",
    "                    np.sqrt(mem + 1e-8)  # adagrad update\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('sample at epoch:', epoch, 'with loss of:', loss)\n",
    "        print(sample(inputs[0], h[-1:][0], 200), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "_Notebook title_ <br/>\n",
    "by _Author (provide a link if possible)_ <br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Benjamin Voigt, Steven Mi\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
