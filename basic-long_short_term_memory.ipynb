{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Long Short Term Memory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\">Introduction</a></span></li><li><span><a href=\"#Requirements\" data-toc-modified-id=\"Requirements-2\">Requirements</a></span><ul class=\"toc-item\"><li><span><a href=\"#Knowledge\" data-toc-modified-id=\"Knowledge-2.1\">Knowledge</a></span></li><li><span><a href=\"#Python-Modules\" data-toc-modified-id=\"Python-Modules-2.2\">Python Modules</a></span></li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2.3\">Data</a></span></li></ul></li><li><span><a href=\"#Recap:-Recurrent-Neural-Network\" data-toc-modified-id=\"Recap:-Recurrent-Neural-Network-3\">Recap: Recurrent Neural Network</a></span></li><li><span><a href=\"#Long-Short-Term-Memory\" data-toc-modified-id=\"Long-Short-Term-Memory-4\">Long Short Term Memory</a></span><ul class=\"toc-item\"><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-4.1\">Implementation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Functions-for-the-LSTM\" data-toc-modified-id=\"Functions-for-the-LSTM-4.1.1\">Functions for the LSTM</a></span></li><li><span><a href=\"#Forward-pass\" data-toc-modified-id=\"Forward-pass-4.1.2\">Forward pass</a></span></li><li><span><a href=\"#Backward-path\" data-toc-modified-id=\"Backward-path-4.1.3\">Backward path</a></span></li><li><span><a href=\"#Sampling\" data-toc-modified-id=\"Sampling-4.1.4\">Sampling</a></span></li><li><span><a href=\"#Trainings-process\" data-toc-modified-id=\"Trainings-process-4.1.5\">Trainings process</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameter\" data-toc-modified-id=\"Hyperparameter-4.1.5.1\">Hyperparameter</a></span></li><li><span><a href=\"#Main\" data-toc-modified-id=\"Main-4.1.5.2\">Main</a></span></li></ul></li><li><span><a href=\"#Learning-curve\" data-toc-modified-id=\"Learning-curve-4.1.6\">Learning curve</a></span></li></ul></li></ul></li><li><span><a href=\"#Licenses\" data-toc-modified-id=\"Licenses-5\">Licenses</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notebook-License-(CC-BY-SA-4.0)\" data-toc-modified-id=\"Notebook-License-(CC-BY-SA-4.0)-5.1\">Notebook License (CC-BY-SA 4.0)</a></span></li><li><span><a href=\"#Code-License-(MIT)\" data-toc-modified-id=\"Code-License-(MIT)-5.2\">Code License (MIT)</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Knowledge\n",
    "\n",
    "- [Recommended] [Neural Network and Deep Learning - Backpropagation](http://neuralnetworksanddeeplearning.com/chap2.html)\n",
    "- [Recommended] [Matrix Calculus](https://explained.ai/matrix-calculus/index.html)\n",
    "- [Recommended] [cs231 Recurrent Neural Network](https://www.youtube.com/watch?v=6niqTuYFZLQ)\n",
    "- [Recommended] [Basics of Recurrent Neural Network Notebook]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('data.txt', 'r').read()\n",
    "\n",
    "text_length = len(text)\n",
    "chars = list(set(text))\n",
    "char_length = len(chars)\n",
    "\n",
    "# dictionaries which we will use in the future for transformations\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# transform the text string to a list of integers\n",
    "X = np.array([char_to_int[char] for char in text])\n",
    "\n",
    "print('text:\\n', text[:200], '\\n')\n",
    "print('length of text:\\n', text_length, '\\namount of characters:\\n', char_length)\n",
    "print('alphabet:\\n', chars,'\\n')\n",
    "print('first 10 datas:\\n', X[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Recurrent Neural Network\n",
    "\n",
    "**[[Summary from colah's blog]](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)** Humans don’t start their thinking from scratch every second. As you read this, you understand each word based on your understanding of previous words. You don’t throw everything away and start thinking from scratch again. Your thoughts have persistence. For example, imagine you want to classify what kind of event is happening at every point in a movie. It’s unclear how a traditional neural network could use its reasoning about previous events in the film to inform later ones.\n",
    "\n",
    "Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist. The idea is, that they might be able to connect previous information to the present task, such as using previous video frames might inform the understanding of the present frame. \n",
    "\n",
    "But in theory, RNNs are absolutely capable of handling “long-term dependencies\". In practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991) and Bengio, et al. (1994), who found some pretty fundamental reasons why it might be difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long Short Term Memory\n",
    "Long Short Term Memory networks are Recurrent Neural Networks capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997). They work tremendously well on a large variety of problems, and are now widely used.\n",
    "\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is their default behavior, not something they struggle to learn! Lets have a look at a single LSTM cell:\n",
    "\n",
    "<img src=\"images/lstm-diagram.jpg\" width=\"400px\">\n",
    "\n",
    "- $f$ forget gate, whether to erase the cell\n",
    "- $i$ input gate, whether to write to cell\n",
    "- $g$ gate gate, how much to write to a cell\n",
    "- $o$ output gate, how much to reveal a cell\n",
    "- $c_t$ cell state at a time $t$\n",
    "- $h_t$ hidden state at a time $t$\n",
    "- $y_t$ output at a time $t$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "Here we go over the core functions of a recurrent neural network. Every function could be added to a ```lstm.py``` script to run the long short term memory network outside of a notebook. This implementation is not efficient though and it is not recommended to use it on a very large text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions for the LSTM\n",
    "The long short term memory uses the sigmoid and the tanh function. Instead of hard-coding the functions into the forward method, we use helper/utility methods for a much cleaner code. You can imagine these snippets inside a ```utility.py``` or as private methods of ```lstm.py```.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}} && \\sigma^\\prime(x) = \\sigma(x) (1 - \\sigma(x))\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid(x):\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    return out * (1 - out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\tanh(x) && \\tanh^\\prime(x) = 1 - \\tanh^2(x)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def dtanh(x):\n",
    "    return 1 + np.tanh(x) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward pass\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \\left( \\begin{array} { c } { i } \\\\{ f } \\\\ { o } \\\\ { g } \\end{array} \\right) & = \\left( \\begin{array} { c } { \\sigma } \\\\ { \\sigma } \\\\ { \\sigma } \\\\ { \\tanh } \\end{array} \\right) W \\left( \\begin{array} { c } { h _ { t - 1 } } \\\\ { x _ { t } } \\end{array} \\right) \\\\ c _ { t } & = f \\odot c _ { t - 1 } + i \\odot g \\\\ h _ { t } & = o \\odot \\tanh \\left( c _ { t } \\right) \\\\\n",
    "y_t &= W_{y} * h_t \\\\\n",
    "p_t &=\\frac{e^{y_t}}{\\sum_k e^{y_k}} \\\\\n",
    "L &= \\sum_t - log (p_t)\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_size = 25\n",
    "stack_size = hidden_size + char_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights of the gates\n",
    "Wf = np.random.randn(hidden_size, stack_size) * 0.01\n",
    "Wi = np.random.randn(hidden_size, stack_size) * 0.01\n",
    "Wg = np.random.randn(hidden_size, stack_size) * 0.01\n",
    "Wo = np.random.randn(hidden_size, stack_size) * 0.01\n",
    "\n",
    "# bias of the gates\n",
    "bf = np.zeros((hidden_size, 1)) * 0.01\n",
    "bi = np.zeros((hidden_size, 1)) * 0.01\n",
    "bg = np.zeros((hidden_size, 1)) * 0.01\n",
    "bo = np.zeros((hidden_size, 1)) * 0.01\n",
    "\n",
    "Wy = np.random.randn(char_length, hidden_size) * 0.01\n",
    "by = np.zeros((char_length, 1)) * 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, Y, hprev, cprev):\n",
    "    # initializing our variables\n",
    "    h, c, p, loss = [hprev], [cprev], [], 0\n",
    "    f, i, g, o = [], [], [], []\n",
    "    for t in range(len(X)):\n",
    "        # transforming the a datapoint at a time step to a one hot encoded vector\n",
    "        xt = np.zeros((char_length, 1))\n",
    "        xt[X[t]] = 1\n",
    "        # stack the hidden state and data point together\n",
    "        stacked = np.append(h[t], xt)\n",
    "        stacked = stacked.reshape(stacked.shape[0], 1)\n",
    "        # get the 4 gates described at the diagram\n",
    "        f.append(sigmoid(Wf @ stacked + bf))\n",
    "        i.append(sigmoid(Wi @ stacked + bi))\n",
    "        g.append(sigmoid(Wo @ stacked + bg))\n",
    "        o.append(tanh(Wg @ stacked + bo))\n",
    "        # calculate the new cell and hidden state\n",
    "        c.append(c[t] * f[t] + i[t] * g[t])\n",
    "        h.append(tanh(c[t + 1]) * o[t])\n",
    "        # calculate the out/prediction for the input data based on the current hidden state\n",
    "        y = np.dot(Wy, h[t + 1])\n",
    "        p.append(np.exp(y) / np.sum(np.exp(y)))\n",
    "        # calculate loss of the prediction\n",
    "        loss += -np.sum(np.log(p[t][Y[t], 0]))\n",
    "    return h, c, f, i, g, o, p, loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward path\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L_t}{\\partial W_{hy}} = (p_t - label_t) *h_t^T  \\\\ \\frac{\\partial L_t}{\\partial h_t} = (p_t - label_t) * W_{hy}^T\\\\\n",
    "\\frac{\\partial h_t}{\\partial c_t} = o \\odot \\tanh^\\prime(c_t) & \\quad \\frac{\\partial h_t}{\\partial o_t} = \\tanh(c_t)\n",
    "\\\\\n",
    "\\frac{\\partial c_t}{\\partial f_t} = c_{t-1} \\quad \\frac{\\partial c_t}{\\partial g_t} = i  \\quad \\frac{\\partial c_t}{\\partial i_t} = g &\n",
    "\\\\\n",
    "\\frac{\\partial f_t}{\\partial W} =\\sigma^\\prime \\text{stack}_t^T \\quad \\frac{\\partial i_t}{W} = \\sigma^\\prime \\text{stack}_t^T\\quad \\frac{\\partial g_t}{\\partial W} = \\tanh^\\prime \\text{stack}_t^T &\\quad \\frac{\\partial o_t}{\\partial W} = \\sigma^\\prime \\text{stack}_t^T\\\\\n",
    "\\frac{\\partial h_t}{\\partial h_{t-1}}=  &\\quad \\frac{\\partial c_t}{\\partial c_{t-1}} = \n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, Y, h, c, f, i, g, o, p):\n",
    "    # initializes the gradients with zeros\n",
    "    dWf, dWi, dWo, dWg, dWy = np.zeros_like(Wf), np.zeros_like(\n",
    "        Wi), np.zeros_like(Wo), np.zeros_like(Wg), np.zeros_like(Wy)\n",
    "    dbf, dbi, dbg, dbo, dby = np.zeros_like(bf), np.zeros_like(\n",
    "        bi), np.zeros_like(bg), np.zeros_like(bo), np.zeros_like(by)\n",
    "\n",
    "    dhprevious = np.zeros_like(h[0])\n",
    "    dcprevious = np.zeros_like(c[0])\n",
    "    for t in reversed(range(len(X))):\n",
    "        # transforming the a datapoint at a time step to a one hot encoded vector\n",
    "        xt = np.zeros((char_length, 1))\n",
    "        xt[X[t]] = 1\n",
    "        # stack the hidden state and data point together\n",
    "        stacked = np.append(h[t], xt)\n",
    "        stacked = stacked.reshape(stacked.shape[0], 1)\n",
    "        # gradient of Why\n",
    "        dy = np.copy(p[t])\n",
    "        dy[Y[t]] -= 1\n",
    "        dby += dy\n",
    "        dWy += np.dot(dy, h[t].T)\n",
    "        # gradient of Wf, Wi, Wg, Wo\n",
    "        dh = np.dot(Wy.T, dy) + dhprevious  # backprop into h\n",
    "        # derivative of the hidden state with respect to the cell state \n",
    "        dc = o[t] * dtanh(c[t]) * dh + dcprevious\n",
    "        # derivative of the hidden state with respect to the output cate\n",
    "        do = dh * tanh(c[t]) * dsigmoid(o[t])\n",
    "        # derivative of the cell state with respect to the gates\n",
    "        df = dc * c[t-1] * dsigmoid(f[t])\n",
    "        di = dc * g[t] * dsigmoid(i[t])\n",
    "        dg = dc * i[t]\n",
    "        dg = dg * dtanh(dg)\n",
    "        # derivative of the gates with respect to the weights\n",
    "        dbf += df\n",
    "        dWf += np.dot(df, stacked.T) \n",
    "        dbi += di\n",
    "        dWi += np.dot(di, stacked.T)\n",
    "        dbg += dg\n",
    "        dWg += np.dot(dg, stacked.T)\n",
    "        dbo += do\n",
    "        dWo += np.dot(do, stacked.T)\n",
    "        # gradient flow\n",
    "        dhprevious = np.dot(Wf.T, df) + np.dot(Wi.T, di) + \\\n",
    "            np.dot(Wo.T, dc) + np.dot(Wg.T, dg)\n",
    "        dhprevious = dhprevious[:hidden_size, :]\n",
    "        dcprevious = dc * f[t]\n",
    "\n",
    "    # gradient clip to mitigate exploding gradients\n",
    "    for dparam in [dWf, dWi, dWg, dWo, dWy, dbf, dbi, dbg, dbo, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "    return dWf, dWi, dWg, dWo, dWy, dbf, dbi, dbg, dbo, dby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(start_char, h, c, n):\n",
    "    '''\n",
    "    This functions returns a sentence of length n based on the starting character, the latest hidden state.\n",
    "\n",
    "    Args:\n",
    "        start_char: the character which we start with as integer\n",
    "        h: latest hidden state\n",
    "        n: length of sentence as integer\n",
    "\n",
    "    Returns:\n",
    "        A sample sentence as String\n",
    "    '''\n",
    "    # transforming the a starting character to a one hot encoded vector\n",
    "    xt = np.zeros((char_length, 1))\n",
    "    xt[start_char] = 1\n",
    "    # initializing output\n",
    "    text = ''\n",
    "    for t in range(n):\n",
    "        # stack the hidden state and data point together\n",
    "        stacked = np.append(h, xt)\n",
    "        stacked = stacked.reshape(stacked.shape[0], 1)\n",
    "        # get the 4 gates described at the diagram\n",
    "        f = sigmoid(Wf @ stacked + bf)\n",
    "        i = sigmoid(Wi @ stacked + bi)\n",
    "        g = sigmoid(Wo @ stacked + bg)\n",
    "        o = tanh(Wg @ stacked + bo)\n",
    "        # calculate the new cell and hidden state\n",
    "        c = (c * f + i * g)\n",
    "        h = tanh(c) * o\n",
    "        # calculate the out/prediction for the input data based on the current hidden state\n",
    "        y = np.dot(Wy, h)\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        # adds the predicted character to our output String\n",
    "        text += chars[np.argmax(p)]\n",
    "        # generates a random sample/new character from a given array based on a probability distributipn p\n",
    "        random_index = np.random.choice(range(char_length), p=p.ravel())\n",
    "        # transforming the generated character to a one hot encoded vector\n",
    "        xt = np.zeros((char_length, 1))\n",
    "        xt[random_index] = 1\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainings process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate for the gradient descent algorithm\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# how many times the model sees the whole data\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_squared_Wf, grad_squared_Wi, grad_squared_Wg, grad_squared_Wo, grad_squared_Wy = np.zeros_like(\n",
    "    Wf), np.zeros_like(Wi), np.zeros_like(Wg), np.zeros_like(Wo), np.zeros_like(Wy)\n",
    "\n",
    "grad_squared_bf, grad_squared_bi, grad_squared_bg, grad_sqared_bo, grad_squared_by = np.zeros_like(bf), np.zeros_like(bi), np.zeros_like(bg), np.zeros_like(bo), np.zeros_like(by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing hidden state, loss history, squared gradient and loss\n",
    "loss_history = []\n",
    "h, c = 0, 0\n",
    "loss = 0\n",
    "steps = 0\n",
    "smooth_loss = -np.log(1.0 / text_length) * seq_size\n",
    "\n",
    "for iteration in tqdm(range(iterations)):\n",
    "    if steps+seq_size+1 >= text_length or iteration == 0:\n",
    "        h = [np.zeros((hidden_size, 1))]\n",
    "        c = [np.zeros((hidden_size, 1))]\n",
    "        steps = 0\n",
    "    # splits the data to the right sequence size\n",
    "    inputs = X[steps:steps+seq_size]\n",
    "    targets = X[steps+1:steps+1+seq_size]\n",
    "    # calculate the new hidden state, probability distribution and loss\n",
    "    h, c, f, i, g, o, p, loss = forward(inputs, targets, h[-1], c[-1])\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    loss_history.append(smooth_loss)\n",
    "    # get the gradients\n",
    "    dWf, dWi, dWg, dWo, dWy, dbf, dbi, dbg, dbo, dby = backward(inputs, targets, h, c, f, i, g, o, p)\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wf, Wi, Wg, Wo, Wy, bf, bi, bg, bo, by],\n",
    "                                  [dWf, dWi, dWg, dWo, dWy, dbf, dbi, dbg, dbo, dby],\n",
    "                                  [grad_squared_Wf, grad_squared_Wi, grad_squared_Wg, grad_squared_Wo, grad_squared_Wy, grad_squared_bf, grad_squared_bi, grad_squared_bg, grad_sqared_bo, grad_squared_by]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam #/ \\\n",
    "            #np.sqrt(mem + 1e-8)  # adagrad update\n",
    "\n",
    "    if iteration % 10 == 0:\n",
    "        print('sample at iteration:', iteration, 'with loss of:', smooth_loss)\n",
    "        print(sample(inputs[0], h[-1], c[-1], 150), '\\n')\n",
    "\n",
    "    steps += seq_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Licenses\n",
    "\n",
    "### Notebook License (CC-BY-SA 4.0)\n",
    "\n",
    "*The following license applies to the complete notebook, including code cells. It does however not apply to any referenced external media (e.g., images).*\n",
    "\n",
    "_Notebook title_ <br/>\n",
    "by _Author (provide a link if possible)_ <br/>\n",
    "is licensed under a [Creative Commons Attribution-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-sa/4.0/).<br/>\n",
    "Based on a work at https://gitlab.com/deep.TEACHING.\n",
    "\n",
    "\n",
    "### Code License (MIT)\n",
    "\n",
    "*The following license only applies to code cells of the notebook.*\n",
    "\n",
    "Copyright 2018 Benjamin Voigt, Steven Mi\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
